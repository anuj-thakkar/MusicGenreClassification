{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the cleaned dataset\n",
    "df_clean = pd.read_csv('data/clean/features_30_sec_pca.csv')\n",
    "df_clean.head()\n",
    "\n",
    "# Splitting the predictor value from the remainder of the dataset\n",
    "\n",
    "X = df_clean.drop(['label'], axis=1)\n",
    "y = df_clean['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "from statistics import mean\n",
    "\n",
    "\n",
    "# Creating a reusable function for churning through all five binary classification algorithms\n",
    "def generate_binary_classification_model(X, y, model_algorithm, hyperparameters, needs_scaled = False):\n",
    "    \"\"\"\n",
    "    Generating everything required for training and validation of a binary classification model\n",
    "\n",
    "    Args:\n",
    "        - X (Pandas DataFrame): A DataFrame containing the cleaned training data\n",
    "        - y (Pandas DataFrame): A DataFrame containing the target values correlated to the X training data\n",
    "        - model_algorithm (object): A model algorithm that will be trained against the X and y data\n",
    "        - hyperparameters (dict): A dictionary containing all the hyperparameters to test the model with\n",
    "        - needs_scaled (Boolean): A boolean value that indicates whether or not the input dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Performing a scaling on the data if required\n",
    "    if needs_scaled:\n",
    "\n",
    "        # Instantiating the StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        # Performing a fit_transform on the dataset\n",
    "        scaled_features = scaler.fit_transform(X)\n",
    "\n",
    "        # Transforming the StandardScaler output back into a Pandas DataFrame\n",
    "        X = pd.DataFrame(scaled_features, index = X.index, columns = X.columns)\n",
    "\n",
    "    # Instantiating a GridSearch object with the inputted model algorithm and hyperparameters\n",
    "    gridsearchcv = GridSearchCV(estimator = model_algorithm,\n",
    "                                param_grid = hyperparameters)\n",
    "\n",
    "    # Fitting the training data to the GridSearch object\n",
    "    gridsearchcv.fit(X, y)\n",
    "\n",
    "    # Printing out the best hyperparameters\n",
    "    print(f'Best hyperparameters: {gridsearchcv.best_params_}')\n",
    "\n",
    "    # Instantiating a new model object with the ideal hyperparameters from the GridSearch job\n",
    "    model_algorithm.set_params(**gridsearchcv.best_params_)\n",
    "\n",
    "    # Creating a container to hold each set of validation metrics\n",
    "    accuracy_scores, roc_auc_scores, f1_scores = [], [], []\n",
    "\n",
    "    # Instantiating the K-Fold cross validation object\n",
    "    k_fold = KFold(n_splits = 5)\n",
    "\n",
    "    # Iterating through each of the folds in K-Fold\n",
    "    for train_index, val_index in k_fold.split(X):\n",
    "\n",
    "        # Splitting the training set from the validation set for this specific fold\n",
    "        X_train, X_val = X.iloc[train_index, :], X.iloc[val_index, :]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        # Fitting the X_train and y_train datasets to the model algorithm\n",
    "        model_algorithm.fit(X_train, y_train)\n",
    "\n",
    "        # Getting inferential predictions for the validation dataset\n",
    "        val_preds = model_algorithm.predict(X_val)\n",
    "\n",
    "        # Generating validation metrics by comparing the inferential predictions (val_preds) to the actuals (y_val)\n",
    "        val_accuracy = accuracy_score(y_val, val_preds)\n",
    "        val_roc_auc_score = roc_auc_score(y_val, val_preds, average = 'macro')\n",
    "        val_f1_score = f1_score(y_val, val_preds, average='macro')\n",
    "\n",
    "        # Appending the validation scores to the respective validation metric container\n",
    "        accuracy_scores.append(val_accuracy)\n",
    "        roc_auc_scores.append(val_roc_auc_score)\n",
    "        f1_scores.append(val_f1_score)\n",
    "\n",
    "    # Getting the average (mean) of each validation score\n",
    "    average_accuracy = int(mean(accuracy_scores) * 100)\n",
    "    average_roc_auc_score = int(mean(roc_auc_scores) * 100)\n",
    "    average_f1_score = int(mean(f1_scores) * 100)\n",
    "\n",
    "    # Printing out the average validation metrics\n",
    "    print(f'Average accuracy score: {average_accuracy}%')\n",
    "    print(f'Average ROC AUC score: {average_roc_auc_score}%')\n",
    "    print(f'Average F1 score: {average_f1_score}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.68\n",
      "F1 score: 0.6756053709976788\n"
     ]
    }
   ],
   "source": [
    "# MODEL1: Logistic Regression\n",
    "# Instantiating a LogisticRegression object\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Setting the hyperparameter grid for the Logistic Regression algorithm\n",
    "logistic_reg_params = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': np.logspace(-4, 4, 20),\n",
    "    'solver': ['lbfgs', 'liblinear']\n",
    "}\n",
    "\n",
    "\n",
    "X = df_clean.drop(['label'], axis=1)\n",
    "y = df_clean['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "\n",
    "# train the model\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# predict the test data\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "\n",
    "# print the accuracy score\n",
    "print(f'Accuracy score: {accuracy_score(y_test, y_pred)}')\n",
    "\n",
    "# F1 score\n",
    "print(f'F1 score: {f1_score(y_test, y_pred, average=\"macro\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.675\n",
      "F1 score: 0.6702480229608713\n"
     ]
    }
   ],
   "source": [
    "# Model 2: Support Vector Machine\n",
    "# Instantiating a Support Vector Machine object\n",
    "svm = SVC()\n",
    "\n",
    "# Setting the hyperparameter grid for the Support Vector Machine algorithm\n",
    "svm_params = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# predict the test data\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "\n",
    "# print the accuracy score\n",
    "print(f'Accuracy score: {accuracy_score(y_test, y_pred)}')\n",
    "\n",
    "# F1 score\n",
    "print(f'F1 score: {f1_score(y_test, y_pred, average=\"macro\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8049262544042122"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 3: XGBoost Classifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Create a classifier\n",
    "xgb = XGBClassifier(booster='gbtree', objective='multi:softprob', random_state=42, eval_metric=\"auc\", num_class=10)\n",
    "\n",
    "# Fit the classifier with the training data\n",
    "xgb.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Use trained model to predict output of test dataset\n",
    "val = xgb.predict(X_test)\n",
    "\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(y_test)\n",
    "\n",
    "y_test_lb = lb.transform(y_test)\n",
    "val_lb = lb.transform(val)\n",
    "\n",
    "roc_auc_score(y_test_lb, val_lb, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.67\n",
      "F1 score: 0.6547855209640339\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# MODEL4: Random Forest\n",
    "# Instantiating a RandomForestClassifier object\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Creating a hyperparameter grid for RandomForestClassifier\n",
    "rfc_hyperparameters = {'n_estimators': [100, 200, 300], \n",
    "                          'max_depth': [None, 5, 10, 20, 30],\n",
    "                            'min_samples_split': [2, 5, 10],\n",
    "                            'min_samples_leaf': [1, 2, 4]}\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# predict the test data\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "\n",
    "# print the accuracy score\n",
    "print(f'Accuracy score: {accuracy_score(y_test, y_pred)}')\n",
    "\n",
    "# F1 score\n",
    "print(f'F1 score: {f1_score(y_test, y_pred, average=\"macro\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.63\n",
      "F1 score: 0.6232271905274428\n"
     ]
    }
   ],
   "source": [
    "# MODEL5: KNN\n",
    "# Instantiating a KNeighborsClassifier object\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Creating a hyperparameter grid for KNeighborsClassifier\n",
    "knn_hyperparameters = {'n_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "                          'weights': ['uniform', 'distance'],   \n",
    "                            'algorithm': ['auto', 'ball_tree', 'kd_tree'],\n",
    "                            'p': [1, 2]}\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# predict the test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "\n",
    "# print the accuracy score\n",
    "print(f'Accuracy score: {accuracy_score(y_test, y_pred)}')\n",
    "\n",
    "# F1 score\n",
    "print(f'F1 score: {f1_score(y_test, y_pred, average=\"macro\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL6: Convolutional Neural Network\n",
    "# Importing the required libraries\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating a Sequential object\n",
    "model = Sequential()\n",
    "\n",
    "# Adding the first convolutional layer\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "\n",
    "# Adding the second convolutional layer\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# Adding the third convolutional layer\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "\n",
    "# Adding the fourth convolutional layer\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "\n",
    "# Adding the max pooling layer\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Adding the flatten layer\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Adding the first dense layer\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
